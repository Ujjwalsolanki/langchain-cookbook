{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e552dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables from a .env file. 🌍\n",
    "# This securely loads API keys (like OPENAI_API_KEY) from an environment file,\n",
    "# preventing them from being hardcoded in the script.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2320fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template. 📝\n",
    "# This template takes a single input variable 'topic' and instructs the LLM\n",
    "# to generate 5 interesting facts about it.\n",
    "prompt = PromptTemplate(\n",
    "    template='Generate 3 interesting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3e849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. 🤖\n",
    "# This creates an instance of the OpenAI chat model (e.g., gpt-3.5-turbo or gpt-4).\n",
    "# This model will be used to generate the facts.\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63a76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a StrOutputParser. 📄\n",
    "# This parser simply extracts the raw string content from the LLM's response.\n",
    "# It ensures that the output of the model is a clean string.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6f0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LangChain Expression Language (LCEL) chain. 🔗\n",
    "# The '|' operator pipes the output of one component as the input to the next.\n",
    "# 1. `prompt`: Takes the `{'topic': 'cricket'}` as input and creates the full prompt string.\n",
    "# 2. `model`: Receives the prompt from `prompt` and generates a text response (the 5 facts).\n",
    "# 3. `parser`: Takes the LLM's raw response (a message object) and extracts its string content.\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a459216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The NFL is the most profitable professional sports league in the world, with an estimated annual revenue of over $15 billion.\n",
      "2. The Super Bowl is the most-watched television event in the United States, with over 100 million viewers tuning in each year.\n",
      "3. The NFL was originally called the American Professional Football Association when it was founded in 1920 and did not adopt its current name until 1922.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the input topic 'NFL'. 🚀\n",
    "# The entire chain executes sequentially, resulting in a string containing the 3 facts.\n",
    "result = chain.invoke({'topic':'nfl'})\n",
    "\n",
    "# Print the final result (the 3 facts). 📊\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "# Print an ASCII representation of the chain's graph. 📈\n",
    "# `chain.get_graph()` returns a graph object representing the runnable chain.\n",
    "# `.print_ascii()` then visualizes this graph in ASCII art,\n",
    "# showing the flow of data through the components (PromptTemplate -> ChatOpenAI -> StrOutputParser).\n",
    "# This is useful for inspecting and understanding the structure of your LangChain pipelines.\n",
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
