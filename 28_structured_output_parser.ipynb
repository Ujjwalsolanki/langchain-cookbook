{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d5ed05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema # Importing necessary classes\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This securely loads API keys (e.g., HUGGINGFACEHUB_API_TOKEN) from an environment file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5001121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HuggingFaceEndpoint model. ü§ñ\n",
    "# This connects to a specific large language model hosted on the Hugging Face Inference API.\n",
    "# 'google/gemma-2-2b-it' is a smaller instruction-tuned model.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c93f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the HuggingFaceEndpoint with ChatHuggingFace. üí¨\n",
    "# This adapts the raw Hugging Face model for LangChain's chat interface.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1c4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the desired structured output. üìù\n",
    "# ResponseSchema is used by StructuredOutputParser to define the expected fields\n",
    "# and their descriptions.\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5595a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StructuredOutputParser from the defined schema. ‚öôÔ∏è\n",
    "# This parser will generate formatting instructions for the LLM and\n",
    "# then attempt to parse the LLM's raw text output into a Python dictionary\n",
    "# conforming to the 'schema'.\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7d67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template. ‚úçÔ∏è\n",
    "# It includes a placeholder for the 'topic' and importantly,\n",
    "# `format_instruction`: This will be populated by the parser with a detailed\n",
    "# text-based instruction on how the LLM should format its output (e.g., as JSON).\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 facts about {topic}\\n{format_instruction}', # Corrected newline\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa05663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LangChain Expression Language (LCEL) chain. üîó\n",
    "# 1. `template`: Generates the prompt, including formatting instructions.\n",
    "# 2. `model`: Sends the prompt to the HuggingFace LLM and gets a text response.\n",
    "# 3. `parser`: Attempts to parse the raw text response from the LLM into a dictionary\n",
    "#    based on the `schema`.\n",
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5d050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': 'Black holes are regions in spacetime where gravity is so strong that nothing, not even light, can escape.', 'fact_2': 'They are formed when very massive stars die and collapse under their own gravity.', 'fact_3': 'While black holes are invisible, scientists can detect them through their gravitational effects on nearby objects, like stars orbiting them.'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the topic 'black hole'. üöÄ\n",
    "# The LLM will receive instructions to output 3 facts in a specific structured format.\n",
    "# However, `google/gemma-2-2b-it` (and similar smaller open-source models) are\n",
    "# generally not reliably able to produce perfectly structured JSON from text-based\n",
    "# formatting instructions. They often generate free-form text, which will cause\n",
    "# the `StructuredOutputParser` to fail or produce an incomplete result.\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "# Print the result. üìä\n",
    "# If the parsing is successful, it will print a dictionary like:\n",
    "# {'fact_1': '...', 'fact_2': '...', 'fact_3': '...'}\n",
    "# However, it's highly probable that this will either:\n",
    "# 1. Raise a parsing error if the LLM's output is not valid JSON.\n",
    "# 2. Return an empty or incomplete dictionary if the parser can't find the structure.\n",
    "# You'd typically need more robust error handling or use models with native structured output capabilities\n",
    "# for reliable results with this pattern.\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
