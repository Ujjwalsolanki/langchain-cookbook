{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6073285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02ba74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple mock LLM class.\n",
    "# This class simulates a Language Model that takes a prompt and returns a random predefined response.\n",
    "# In a real-world scenario, this would be an actual LLM integration (e.g., OpenAI, HuggingFace).\n",
    "class FakeLLM:\n",
    "    def __init__(self):\n",
    "        print('LLM created') # Prints a message when an instance of FakeLLM is created.\n",
    "\n",
    "    # The 'predict' method simulates the LLM generating a response based on the input prompt.\n",
    "    def predict(self, prompt):\n",
    "        # A predefined list of possible responses.\n",
    "        response_list = [\n",
    "            'Paris is the capital of France',\n",
    "            'NFL is a popular league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "        # Randomly selects one of the responses from the list.\n",
    "        # In a real LLM, this would involve sending the prompt to the LLM API and getting its generated text.\n",
    "        return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e08298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple mock PromptTemplate class.\n",
    "# This class simulates how a PromptTemplate works, taking a template string and input variables.\n",
    "# It formats the template with provided values.\n",
    "class FakePromptTemplate:\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = template # The raw template string (e.g., 'Write a {length} poem about {topic}').\n",
    "        self.input_variables = input_variables # A list of variable names expected in the template.\n",
    "\n",
    "    # The 'format' method takes a dictionary of input values and formats the template string.\n",
    "    def format(self, input_dict):\n",
    "        # Uses string's format method to replace placeholders (e.g., {length}, {topic})\n",
    "        # with values from the input_dict.\n",
    "        return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bea41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example of using FakePromptTemplate and FakeLLM individually ---\n",
    "# Create an instance of FakePromptTemplate with a specific template and variables.\n",
    "template = FakePromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88403bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt: Write a short poem about USA\n"
     ]
    }
   ],
   "source": [
    "# Format the template with actual values for 'length' and 'topic'.\n",
    "prompt = template.format({'length':'short','topic':'USA'})\n",
    "print(f\"Formatted Prompt: {prompt}\") # Output: Formatted Prompt: Write a short poem about USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab15c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of FakeLLM.\n",
    "llm = FakeLLM() # Prints 'LLM created'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dbf865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Raw Response: {'response': 'Paris is the capital of France'}\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction using the FakeLLM with the formatted prompt.\n",
    "# This simulates calling an LLM API.\n",
    "llm_response = llm.predict(prompt)\n",
    "print(f\"LLM Raw Response: {llm_response}\") # Output: LLM Raw Response: {'response': '...' (one of the random choices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6165ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Introducing FakeLLMChain to combine PromptTemplate and LLM ---\n",
    "\n",
    "# Define a simple mock LLMChain class.\n",
    "# This class simulates a basic LangChain LLMChain, which orchestrates the flow\n",
    "# from formatting a prompt to getting a prediction from an LLM.\n",
    "class FakeLLMChain:\n",
    "    def __init__(self, llm, prompt):\n",
    "        self.llm = llm     # Stores the LLM instance (e.g., FakeLLM).\n",
    "        self.prompt = prompt # Stores the PromptTemplate instance (e.g., FakePromptTemplate).\n",
    "\n",
    "    # The 'run' method executes the chain's logic.\n",
    "    # It takes a dictionary of input variables required by the prompt template.\n",
    "    def run(self, input_dict):\n",
    "        # 1. Format the prompt template using the provided input dictionary.\n",
    "        final_prompt = self.prompt.format(input_dict)\n",
    "        print(f\"\\nChain - Final Prompt Sent to LLM: {final_prompt}\")\n",
    "\n",
    "        # 2. Make a prediction using the LLM with the formatted prompt.\n",
    "        result = self.llm.predict(final_prompt)\n",
    "        print(f\"Chain - LLM's Full Response: {result}\")\n",
    "\n",
    "        # 3. Extract and return the actual response text from the LLM's raw output.\n",
    "        return result['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af59bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example of using FakeLLMChain ---\n",
    "# IMPORTANT: The provided code had a typo 'NakliPromptTemplate'.\n",
    "# We assume it's meant to be 'FakePromptTemplate' as defined above.\n",
    "# If 'NakliPromptTemplate' was intended to be a separate class, it would need to be defined.\n",
    "# For this explanation, we'll use FakePromptTemplate.\n",
    "template_for_chain = FakePromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016176eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm_for_chain = FakeLLM() # Prints 'LLM created'\n",
    "chain = FakeLLMChain(llm_for_chain, template_for_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01089789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain - Final Prompt Sent to LLM: Write a short poem about india\n",
      "Chain - LLM's Full Response: {'response': 'Paris is the capital of France'}\n",
      "Chain - Final Result: Paris is the capital of France\n"
     ]
    }
   ],
   "source": [
    "# Run the chain with specific input variables.\n",
    "# The chain handles formatting the prompt and getting the response from the LLM internally.\n",
    "chain_result = chain.run({'length':'short', 'topic': 'india'})\n",
    "print(f\"Chain - Final Result: {chain_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82affcc8",
   "metadata": {},
   "source": [
    "## Understanding `FakeLLMChain` and its Parallel with LangChain's `LLMChain` ü§ù\n",
    "\n",
    "Dear Students,\n",
    "\n",
    "The `FakeLLMChain` class you see above is a simplified, custom-made example that demonstrates the core concept behind **LangChain's `LLMChain`**. While it uses \"fake\" components, the fundamental workflow is identical to how a real `LLMChain` operates.\n",
    "\n",
    "-----\n",
    "\n",
    "### What is `FakeLLMChain` (and `LLMChain` in LangChain)?\n",
    "\n",
    "At its heart, `FakeLLMChain` (and `LLMChain` in LangChain) serves as an **orchestrator** or a **pipeline** that connects a `PromptTemplate` to an `LLM` (Language Model). It automates the common two-step process of:\n",
    "\n",
    "1.  **Formatting a prompt**: Taking user inputs and inserting them into a predefined prompt template.\n",
    "2.  **Invoking the LLM**: Sending the fully formatted prompt to the Language Model to get a response.\n",
    "\n",
    "-----\n",
    "\n",
    "### Why We Use It (The Benefits) üöÄ\n",
    "\n",
    "Instead of performing these steps manually every time:\n",
    "\n",
    "```python\n",
    "# Manual approach\n",
    "formatted_prompt = my_prompt_template.format(user_input)\n",
    "llm_response = my_llm.invoke(formatted_prompt)\n",
    "parsed_response = my_parser.parse(llm_response)\n",
    "```\n",
    "\n",
    "An `LLMChain` (or `FakeLLMChain`) encapsulates this logic, making your code:\n",
    "\n",
    "  * **Cleaner and More Concise**: You define the chain once, and then simply call `run()` (or `invoke()` in modern LangChain) with your input.\n",
    "  * **More Modular**: It clearly separates the concerns: the prompt template handles formatting, the LLM generates text, and the chain manages the flow between them.\n",
    "  * **Easier to Reuse**: Once a chain is defined, you can reuse it anywhere in your application with different inputs without rewriting the prompt formatting and LLM calling logic.\n",
    "  * **Foundation for Complex Workflows**: This basic chain is the building block for more advanced LangChain constructs like **Sequential Chains**, **Retrieval Chains**, and custom **LangChain Expression Language (LCEL)** pipelines.\n",
    "\n",
    "-----\n",
    "\n",
    "### How It Works (Step-by-Step) üë£\n",
    "\n",
    "Let's break down the `FakeLLMChain`'s `run` method to see the parallel with `LLMChain`:\n",
    "\n",
    "1.  **`final_prompt = self.prompt.format(input_dict)`**:\n",
    "\n",
    "      * This line directly mirrors how `LLMChain` uses its internal `PromptTemplate`. When you call `chain.run({'length':'short', 'topic': 'india'})`, the `input_dict` is passed to the `FakePromptTemplate`'s `format` method.\n",
    "      * The prompt template (`'Write a {length} poem about {topic}'`) gets filled in with `length='short'` and `topic='india'`, resulting in the actual prompt text sent to the LLM.\n",
    "\n",
    "2.  **`result = self.llm.predict(final_prompt)`**:\n",
    "\n",
    "      * This simulates the core interaction with the Language Model. In a real `LLMChain`, this would be where LangChain sends the `final_prompt` to the actual LLM (e.g., OpenAI's API, a HuggingFace model).\n",
    "      * The `FakeLLM` here returns a predefined random response, but imagine a powerful LLM generating a unique poem or factual answer based on your prompt.\n",
    "\n",
    "3.  **`return result['response']`**:\n",
    "\n",
    "      * This step extracts the most relevant part of the LLM's output. Real LLM responses often come as objects or dictionaries with metadata. The chain extracts just the generated text content for you.\n",
    "\n",
    "In essence, `FakeLLMChain` simplifies the interaction with `FakeLLM` and `FakePromptTemplate`, just as `LLMChain` (and more generally, LCEL chains) simplifies interactions with real LLMs and prompt templates in the LangChain ecosystem. It abstracts away the boilerplate, letting you focus on defining your prompts and what you want the LLM to achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
