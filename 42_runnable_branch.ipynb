{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458ce1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnablePassthrough, RunnableBranch # Core LCEL components\n",
    "\n",
    "# Load environment variables from a .env file. ðŸŒ\n",
    "# This ensures your OpenAI API key is loaded securely from your environment.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef5af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ðŸ¤–\n",
    "# This model will be used for both generating the report and summarizing it.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize a StrOutputParser. ðŸ“„\n",
    "# This parser extracts the raw string content from the LLM's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8666e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template for generating a detailed report. ðŸ“\n",
    "# It takes a 'topic' as input and asks the LLM to write a detailed report.\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# Define the second prompt template for summarizing text. ðŸ“\n",
    "# It takes 'text' as input and instructs the LLM to summarize it.\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb7f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `report_gen_chain`. â›“ï¸\n",
    "# This is a simple sequential chain that:\n",
    "# 1. Takes the initial `topic` input.\n",
    "# 2. Formats it using `prompt1` to create the report generation prompt.\n",
    "# 3. Sends the prompt to the `model` to generate the detailed report.\n",
    "# 4. Uses the `parser` to extract the raw report string.\n",
    "# The output of this chain will be the detailed report text.\n",
    "report_gen_chain = prompt1 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413586ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditional branching logic using `RunnableBranch`. ðŸš¦\n",
    "# `RunnableBranch` allows the workflow to choose a path based on a condition.\n",
    "# It takes a series of (condition, runnable) pairs, and an optional default runnable.\n",
    "branch_chain = RunnableBranch(\n",
    "    # First branch: If the length of the input text (output from report_gen_chain)\n",
    "    # is greater than 300 words, then execute the summarization chain.\n",
    "    # `lambda x: len(x.split()) > 300` is the condition function. `x` is the input\n",
    "    # received by `branch_chain` (which is the detailed report text).\n",
    "    # `prompt2 | model | parser` is the runnable to execute if the condition is true.\n",
    "    (lambda x: len(x.split()) > 300, prompt2 | model | parser),\n",
    "\n",
    "    # Default branch: If the condition above is FALSE (i.e., the report is 300 words or less),\n",
    "    # then `RunnablePassthrough()` is executed.\n",
    "    # `RunnablePassthrough()` simply takes its input and passes it through unchanged.\n",
    "    # So, if the report is short, it won't be summarized; it will be returned as is.\n",
    "    RunnablePassthrough()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97675e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text highlights the longstanding cricket rivalry between India and England, two of the most competitive teams in the world. They have faced each other in numerous Test series, ODIs, and T20 matches, with both sides boasting a strong cricketing history and passionate fan base. The rivalry is not limited to the cricket field, as the two countries have a long history of sporting and cultural exchanges. In recent years, India has had the upper hand in Test cricket, while England has shown success in limited-overs cricket. The matches between the two teams are always fiercely competitive, exciting, and closely contested, with both sides striving for success and supremacy in the cricketing world.\n"
     ]
    }
   ],
   "source": [
    "# Define the final overall chain. ðŸ”—\n",
    "# This chain combines `report_gen_chain` and `branch_chain` sequentially.\n",
    "# 1. `report_gen_chain` runs first, generating the detailed report.\n",
    "# 2. The *output* of `report_gen_chain` (the detailed report text) becomes the *input* to `branch_chain`.\n",
    "# 3. `branch_chain` then evaluates its condition:\n",
    "#    - If the report is long (>300 words), it summarizes it.\n",
    "#    - If the report is short (<=300 words), it passes the original report through.\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "# Invoke the `final_chain` with the initial topic. ðŸš€\n",
    "# The input `{'topic':'Russia vs Ukraine'}` starts the entire process.\n",
    "# The output will either be the summarized report or the original detailed report,\n",
    "# depending on its length.\n",
    "print(final_chain.invoke({'topic':'India vs England'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
