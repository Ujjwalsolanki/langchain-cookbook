{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99371007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This securely loads API keys (like OPENAI_API_KEY) from an environment file,\n",
    "# keeping sensitive information out of the main codebase.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1801fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ü§ñ\n",
    "# This creates an instance of the OpenAI chat model (e.g., gpt-3.5-turbo or gpt-4).\n",
    "# This model will be used for both generating the detailed report and summarizing it.\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce7ca1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template for generating a detailed report. üìù\n",
    "# This template takes a single input variable 'topic' and instructs the LLM\n",
    "# to write a detailed report about it.\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a7a2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template for summarizing text. üìù\n",
    "# This template takes a single input variable 'text' and instructs the LLM\n",
    "# to create a 5-line summary of that text.\n",
    "# Note: The '/n' in the original template was likely a typo for '\\n' (newline).\n",
    "# Corrected to '\\n' for proper formatting.\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. \\n {text}',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666ddba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a StrOutputParser. üìÑ\n",
    "# This parser simply extracts the raw string content from the LLM's response.\n",
    "# It's used here to ensure that the output of each model invocation is a clean string\n",
    "# that can be passed as input to the next component in the chain.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d557c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain of operations using LangChain Expression Language (LCEL). üîó\n",
    "# The '|' operator pipes the output of one component as the input to the next.\n",
    "# 1. `template1`: Takes `{'topic': 'black hole'}` as input.\n",
    "# 2. `model`: Receives the prompt from `template1` and generates a detailed report.\n",
    "# 3. `parser`: Takes the LLM's raw response (a message object) and extracts its string content (the detailed report).\n",
    "# 4. `template2`: Receives the detailed report (as 'text') from the parser and creates a new summary prompt.\n",
    "# 5. `model`: Receives the summary prompt from `template2` and generates the 5-line summary.\n",
    "# 6. `parser`: Takes the LLM's raw summary response and extracts its string content.\n",
    "# The final output is the summarized text.\n",
    "# I will explain the chain step by step in later keep following files.\n",
    "chain = template1 | model | parser | template2 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c8647b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5th Test match between India and England in 2025 at Lord's Cricket Ground was the decider in the series with both teams having won two matches each. England won the toss and elected to bat, but the Indian bowlers took crucial wickets, rattling the English batting lineup for 275 runs. India responded with the middle-order steadying the ship, the captain scoring a century, and the lower order contributing to take a significant lead. In the second innings, the Indian spin duo took all ten wickets, bundling out England for a small total. India chased down the target comfortably to win the match and the series 3-2.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the entire chain with the initial input. üöÄ\n",
    "# The input dictionary `{'topic':'black hole'}` is passed to the first component (`template1`),\n",
    "# and the entire chain executes sequentially.\n",
    "result = chain.invoke({'topic':'Ind v Eng test 2025'})\n",
    "\n",
    "# Print the final result of the chain. üìä\n",
    "# This will be the 5-line summary generated by the second LLM call in the chain.\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
