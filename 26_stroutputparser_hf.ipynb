{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e384e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file.\n",
    "# This securely loads any necessary API keys (e.g., for Hugging Face) from a .env file,\n",
    "# keeping sensitive information out of your main script.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70400bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\langchain-cookbook\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFaceEndpoint.\n",
    "# This connects to a specific large language model hosted on the Hugging Face Inference API.\n",
    "# - 'repo_id': Specifies the model to use, here it's 'google/gemma-2-2b-it',\n",
    "#              a conversational model from Google.\n",
    "# - 'task': Defines the primary task the model is intended for, which is \"text-generation\".\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7eb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the HuggingFaceEndpoint with ChatHuggingFace.\n",
    "# ChatHuggingFace adapts the raw Hugging Face model (llm) to work seamlessly\n",
    "# with LangChain's chat-oriented interfaces, allowing for structured message passing.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6454283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template for generating a detailed report. \n",
    "# This template takes a single input variable 'topic' and instructs the LLM\n",
    "# to write a detailed report about it.\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2fcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template for summarizing text.\n",
    "# This template takes a single input variable 'text' and instructs the LLM\n",
    "# to create a 5-line summary of that text.\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. \\n {text}', # Corrected newline character\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed75c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the first prompt template with the topic 'black hole'.\n",
    "# This creates the full prompt string for the detailed report.\n",
    "prompt1 = template1.invoke({'topic':'black hole'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa469377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the first prompt to the HuggingFace model and get the detailed report.\n",
    "# The 'invoke' method sends the prompt and retrieves the model's generated content.\n",
    "result = model.invoke(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a11f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the second prompt template, passing the content of the first result as input.\n",
    "# This creates the full prompt string for the summary, using the generated report as the text to summarize.\n",
    "prompt2 = template2.invoke({'text':result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6523e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the second prompt to the HuggingFace model and get the 5-line summary.\n",
    "# This is the final step where the model summarizes its own previously generated content.\n",
    "result1 = model.invoke(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae530ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are regions of spacetime with such intense gravity that nothing can escape them. They form from the collapse of massive stars or through gravitational mergers, and their properties include mass, spin, event horizon, and a singularity. Black holes cause gravitational lensing, accelerate particle ejection, and generate gravitational waves. While we've learned much about black holes from research, questions about their singularity and the information paradox remain. Research continues with telescopes like the Event Horizon Telescope, revealing further mysteries about these cosmic entities. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the final summary.\n",
    "# This will display the 5-line summary of the black hole report.\n",
    "print(result1.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
