# Let's break down `SystemMessage`, `HumanMessage`, and `AIMessage` in LangChain:

# 1.  **`SystemMessage`**:
#     * **Description**: This message type is used to set the **initial context, persona, or instructions** for the AI. It tells the language model how it should behave or what role it should play *before* the actual conversation begins.
#     * **Why we use it**: It establishes the foundational guidelines for the AI's responses, ensuring consistency in its behavior throughout the chat. For example, `SystemMessage(content='You are a helpful AI assistant')` instructs the model to always be helpful.

# 2.  **`HumanMessage`**:
#     * **Description**: Represents an input or query from the **user** (human) in a conversation. It's how you feed the user's side of the dialogue into the language model.
#     * **Why we use it**: It explicitly marks turns taken by the human participant, allowing the model to understand whose turn it is and to process the current user's request. `chat_history.append(HumanMessage(content=user_input))` adds the user's typed message.

# 3.  **`AIMessage`**:
#     * **Description**: Represents a response generated by the **AI** (model) in the conversation. This is the output from the language model that you then present back to the user.
#     * **Why we use it**: It captures the AI's contribution to the dialogue, allowing its responses to be stored and then provided back to the model in subsequent turns, maintaining a complete conversational history for context. `chat_history.append(AIMessage(content=result.content))` stores the model's reply.

# **In essence, these message types are fundamental building blocks for constructing and maintaining a structured chat history, enabling language models to have stateful, coherent, and contextual conversations.**

## Since we are taking inputs from the user, jupyter notebook is not the best place to run this code.
# Instead, you can run this code in a Python script or an interactive Python shell.
# Below is the complete code for a simple chatbot using LangChain with OpenAI's chat model

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv
import json

# Load environment variables from a .env file.
# This is a crucial step for securely managing API keys (e.g., your OpenAI API key)
# by loading them from an external file instead of hardcoding them in the script.
load_dotenv()

# Initialize the ChatOpenAI language model.
# This creates an instance of the OpenAI chat model (defaults to gpt-3.5-turbo if not specified).
# This 'model' object will be used to send and receive chat messages.
model = ChatOpenAI()

# Initialize the chat history with a SystemMessage.
# chat_history is a list that will store the entire conversation,
# allowing the AI to maintain context.
# A SystemMessage sets the initial behavior or persona of the AI.
chat_history = [
    SystemMessage(content='You are a helpful AI assistant')
]

# Start an infinite loop for continuous conversation.
# The loop will break only when the user types 'exit'.
while True:
    # Get input from the user.
    user_input = input('You: ')

    # Append the user's input as a HumanMessage to the chat history.
    # This records what the user said in the conversation.
    chat_history.append(HumanMessage(content=user_input))

    # Check if the user wants to exit the chat.
    if user_input == 'exit':
        break # Exit the while loop

    # Invoke the language model with the entire chat history.
    # By passing the full chat_history, the model retains memory of previous turns,
    # enabling more coherent and contextual conversations.
    result = model.invoke(chat_history)

    # Append the AI's response as an AIMessage to the chat history.
    # This stores the AI's reply, completing the current turn in the conversation.
    # 'result.content' extracts the actual text from the model's response object.
    chat_history.append(AIMessage(content=result.content))

    # Print the AI's response to the console.
    print("AI: ", result.content)

# After the loop breaks (when user types 'exit'), print the complete chat history.
# This provides a full transcript of the interaction, including system, human, and AI messages.
print("\n--- Full Chat History ---")
print(chat_history)

# Convert chat_history (list of message objects) into a JSON-serializable format
# Each message object (SystemMessage, HumanMessage, AIMessage) has 'type' and 'content' attributes.
serializable_chat_history = []
for message in chat_history:
    serializable_chat_history.append({
        "type": message.type,
        "content": message.content 
    })

# Define the filename for your chat history
output_json_filename = "artifacts/14_chat_history.json"
output_text_filename = "artifacts/14_chat_history.txt"

# Save the serializable chat history to a JSON file
try:
    with open(output_json_filename, 'w', encoding='utf-8') as f:
        json.dump(serializable_chat_history, f, indent=4, ensure_ascii=False)
    print(f"\nChat history saved to {output_json_filename}")

    with open(output_text_filename, 'w') as file: 
        file.write(chat_history)
    print(f"Chat messages successfully dumped to {output_text_filename}")

except Exception as e:
    print(f"\nError saving chat history: {e}")
