{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b4ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13fc4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from a .env file.\n",
    "# This is a best practice for securely managing sensitive information,\n",
    "# such as API keys, keeping them out of the main codebase.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec71c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model.\n",
    "# - 'gpt-4': Specifies the particular OpenAI model to use for chat interactions.\n",
    "#            GPT-4-mini is a powerful model known for its advanced capabilities.\n",
    "#            Here, You can also use other models like 'gpt-3.5-turbo' or 'gpt-4'.\n",
    "# - 'temperature=1.5': Controls the randomness of the model's output.\n",
    "#                      Higher values (like 1.5) make the output more creative and\n",
    "#                      diverse, while lower values make it more deterministic.\n",
    "#                      Also we can choose from 0.0 to 2.0, 0.0 will give you same result every time.\n",
    "# - 'max_completion_tokens=10': Sets the maximum number of tokens (words/pieces of words)\n",
    "#                                the model can generate in its response. This helps\n",
    "#                                control the length of the output.\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=1.5, max_completion_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the language model with a specific prompt.\n",
    "# The prompt asks the model to generate a 5-line poem on the topic of NFL.\n",
    "# The 'invoke' method sends this request to the OpenAI model.\n",
    "result = model.invoke(\"Write a 5 line poem on NFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26f78bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under bright lights, where smashing helmets collide,  \n",
      "Grit and glory on each vibrant stride.  \n",
      "Quarterbacks passing, fans buzzing their cheer,  \n",
      "Strategy and passion, the season draws near.  \n",
      "In the heart of the game, true legends abide\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the result.\n",
    "# The 'invoke' method for ChatOpenAI returns a ChatMessage object,\n",
    "# and the actual generated text is accessed via its '.content' attribute.\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
