{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8e045e",
   "metadata": {},
   "source": [
    "The code defines a simplified LangChain-like system using a custom Runnable abstract base class and concrete implementations for FakeLLM, FakePromptTemplate, FakeStrOutputParser, and a RunnableConnector. It demonstrates how these components can be chained together to create multi-step processing pipelines, mimicking LangChain's LCEL (LangChain Expression Language) for sequential operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e083c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod # For defining Abstract Base Classes\n",
    "import random # For generating random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfe14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Abstract Base Class (ABC) named Runnable. üõ†Ô∏è\n",
    "# This sets the contract for any class that wants to be a \"Runnable.\"\n",
    "# Any class inheriting from Runnable MUST implement the 'invoke' method.\n",
    "class Runnable(ABC):\n",
    "    @abstractmethod # Decorator indicating that 'invoke' must be implemented by subclasses.\n",
    "    def invoke(self, input_data): # 'self' is required for instance methods.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6f82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a FakeLLM class that conforms to the Runnable contract. ü§ñ\n",
    "# This simulates a Language Model, returning random predefined responses.\n",
    "class FakeLLM(Runnable):\n",
    "    def __init__(self):\n",
    "        print('LLM created') # Confirmation that the LLM instance is created.\n",
    "\n",
    "    # The 'invoke' method is the core of a Runnable; it processes input_data.\n",
    "    # Here, it takes a 'prompt' (which is expected to be a string from the previous step)\n",
    "    # and returns a dictionary with a 'response' key, mimicking an LLM API's output.\n",
    "    def invoke(self, prompt):\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "        # In a real LLM, this would send the prompt to an actual LLM service.\n",
    "        return {'response': random.choice(response_list)}\n",
    "\n",
    "    # The 'predict' method is redundant if 'invoke' is the primary interface.\n",
    "    # It serves the same purpose as 'invoke' in this fake setup.\n",
    "    def predict(self, prompt):\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "        return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea410587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a FakePromptTemplate class that conforms to the Runnable contract. üìù\n",
    "# This simulates a prompt template that formats a string with given inputs.\n",
    "class FakePromptTemplate(Runnable):\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = template # The string template (e.g., 'Write a {length} poem about {topic}').\n",
    "        self.input_variables = input_variables # Names of variables in the template.\n",
    "\n",
    "    # The 'invoke' method processes the input dictionary to format the template.\n",
    "    # It expects an 'input_dict' containing values for 'input_variables'.\n",
    "    # It returns the formatted string, ready to be passed to an LLM.\n",
    "    def invoke(self, input_dict):\n",
    "        return self.template.format(**input_dict)\n",
    "\n",
    "    # The 'format' method serves the same purpose as 'invoke' in this fake setup.\n",
    "    def format(self, input_dict):\n",
    "        return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d83197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a FakeStrOutputParser class that conforms to the Runnable contract. üìÑ\n",
    "# This simulates a basic output parser that extracts a string from the LLM's raw output.\n",
    "class FakeStrOutputParser(Runnable):\n",
    "    def __init__(self):\n",
    "        pass # No initialization needed for this simple parser.\n",
    "\n",
    "    # The 'invoke' method takes the LLM's raw output (expected to be a dict with 'response' key)\n",
    "    # and extracts just the string value associated with the 'response' key.\n",
    "    def invoke(self, input_data):\n",
    "        return input_data['response'] # Assumes input_data is a dictionary from FakeLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ef2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a RunnableConnector class that acts as a chain. üîó\n",
    "# This class takes a list of Runnables and executes them sequentially.\n",
    "# It mimics the sequential chaining functionality of LangChain Expression Language (LCEL).\n",
    "class RunnableConnector(Runnable):\n",
    "    def __init__(self, runnable_list):\n",
    "        self.runnable_list = runnable_list # A list of Runnable instances to execute in order.\n",
    "\n",
    "    # The 'invoke' method processes input_data through each runnable in the list.\n",
    "    # The output of one runnable becomes the input for the next.\n",
    "    def invoke(self, input_data):\n",
    "        current_data = input_data # Start with the initial input.\n",
    "        for runnable in self.runnable_list:\n",
    "            # Each runnable's 'invoke' method is called with the output of the previous one.\n",
    "            current_data = runnable.invoke(current_data)\n",
    "        return current_data # Return the final output after all runnables have executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53708e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- First Chain Example ---\n",
    "\n",
    "# Define a prompt template for generating a poem.\n",
    "template = FakePromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19075d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "# Initialize the fake LLM and parser.\n",
    "llm = FakeLLM()\n",
    "parser = FakeStrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30a4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain using RunnableConnector. ‚õìÔ∏è\n",
    "# This chain will:\n",
    "# 1. Take initial input (e.g., {'length':'long', 'topic':'paris'}).\n",
    "# 2. Pass it to 'template' (FakePromptTemplate) to format the prompt.\n",
    "# 3. Pass the formatted prompt to 'llm' (FakeLLM) to get a fake LLM response (dict).\n",
    "# 4. Pass the LLM response to 'parser' (FakeStrOutputParser) to extract the string content.\n",
    "chain = RunnableConnector([template, llm, parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df5116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Chain Result: Delhi is the capital of India\n"
     ]
    }
   ],
   "source": [
    "# Invoke the first chain. üöÄ\n",
    "# This executes the entire pipeline defined by 'chain'.\n",
    "result1 = chain.invoke({'length':'long', 'topic':'paris'})\n",
    "print(f\"First Chain Result: {result1}\") # Will print one of the random LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b49836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Second (Nested) Chain Example ---\n",
    "\n",
    "# Define templates for a joke and its explanation.\n",
    "template1 = FakePromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "template2 = FakePromptTemplate(\n",
    "    template='Explain the following joke {response}', # Note: 'response' matches LLM output key.\n",
    "    input_variables=['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae00a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and parser again (can reuse instances).\n",
    "llm = FakeLLM()\n",
    "parser = FakeStrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e86678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'chain1': Generates a joke. ‚õìÔ∏è\n",
    "# It takes a 'topic', formats it into a joke prompt, and gets a joke from the LLM.\n",
    "# The output of chain1 will be a dictionary like {'response': 'IPL is a cricket league'} (from FakeLLM)\n",
    "# because the parser isn't at the end of chain1. This output structure is crucial for template2.\n",
    "chain1 = RunnableConnector([template1, llm])\n",
    "\n",
    "# Create 'chain2': Explains a joke. ‚õìÔ∏è\n",
    "# It takes the LLM's raw response (e.g., {'response': 'IPL is a cricket league'})\n",
    "# formats it into an explanation prompt using `template2` (which expects a 'response' key),\n",
    "# gets an explanation from the LLM, and then parses it into a string.\n",
    "chain2 = RunnableConnector([template2, llm, parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a90868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'final_chain': A nested chain combining chain1 and chain2. ‚õìÔ∏èüîó\n",
    "# 1. Takes initial input (e.g., {'topic':'cricket'}).\n",
    "# 2. Passes it to 'chain1', which generates a joke (as a dictionary like {'response': '...'}).\n",
    "# 3. The output of 'chain1' (the dictionary) is then passed as input to 'chain2'.\n",
    "#    Crucially, 'chain2' expects a dictionary input that contains a 'response' key,\n",
    "#    which aligns perfectly with 'chain1's output.\n",
    "# 4. 'chain2' then explains the joke and extracts the final string.\n",
    "final_chain = RunnableConnector([chain1, chain2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca1b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Chain Result: AI stands for Artificial Intelligence\n"
     ]
    }
   ],
   "source": [
    "# Invoke the final nested chain. üöÄ\n",
    "result2 = final_chain.invoke({'topic':'cricket'})\n",
    "print(f\"Second Chain Result: {result2}\") # Will print the 'explanation' (another random LLM response)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
