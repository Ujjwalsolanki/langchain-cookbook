{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339fb4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence # Explicitly importing RunnableSequence\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This line ensures your OpenAI API key is loaded securely from your environment,\n",
    "# so it's not hardcoded in the script.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b12a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ü§ñ\n",
    "# This creates an instance of the OpenAI chat model (e.g., gpt-3.5-turbo or gpt-4),\n",
    "# which will be used for both generating the joke and explaining it.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize a StrOutputParser. üìÑ\n",
    "# This parser simply extracts the raw string content from the LLM's response message.\n",
    "# It's used after each model call to ensure the output passed to the next stage is a clean string.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559975f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template. üìù\n",
    "# This template takes a 'topic' as input and instructs the LLM to generate a joke about it.\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a dad-joke about {topic} in one sentence',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66b50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template. üìù\n",
    "# This template takes 'text' (which will be the joke generated in the previous step)\n",
    "# and instructs the LLM to explain that joke.\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text} in 2 sentences',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b8c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential chain using RunnableSequence. üîó\n",
    "# `RunnableSequence` is a way to explicitly define a pipeline where components execute\n",
    "# in a specific order, with the output of one component becoming the input for the next.\n",
    "# It's functionally equivalent to chaining with the `|` operator (LangChain Expression Language).\n",
    "# The sequence of operations is:\n",
    "# 1. `prompt1`: Receives the initial input `{'topic':'AI'}`. It formats the joke prompt.\n",
    "# 2. `model`: Receives the joke prompt from `prompt1` and generates the joke text.\n",
    "# 3. `parser`: Extracts the raw string joke from the model's output.\n",
    "# 4. `prompt2`: Receives the extracted joke text (as `text`) from the parser. It formats the explanation prompt.\n",
    "# 5. `model`: Receives the explanation prompt from `prompt2` and generates the joke's explanation.\n",
    "# 6. `parser`: Extracts the raw string explanation from the model's output.\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465c304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke plays on the double meaning of \"unresolved issues,\" as it can refer to both emotional problems and technical problems within the artificial intelligence. The punchline suggests that the AI sought therapy to address both its emotional and technical issues.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the entire chain with the initial input. üöÄ\n",
    "# The input dictionary `{'topic':'AI'}` is fed into the first component (`prompt1`),\n",
    "# and the entire multi-step process (joke generation then explanation) executes automatically.\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d7121",
   "metadata": {},
   "source": [
    "### Here is an issue, it just printed explanations, it didnot printed joke, that will cover in next file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
