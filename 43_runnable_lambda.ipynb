{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cf6d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel # Core LCEL components\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This ensures your OpenAI API key is loaded securely from your environment.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73009371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ü§ñ\n",
    "# This model will be used for generating the joke.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize a StrOutputParser. üìÑ\n",
    "# This parser extracts the raw string content from the LLM's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc77faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Python function to count words. üêç\n",
    "# This function takes a string `text` and returns the number of words in it.\n",
    "# This demonstrates how you can integrate arbitrary Python logic into LangChain pipelines.\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a438d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for joke generation. üìù\n",
    "# It takes a `topic` as input and instructs the LLM to write a joke.\n",
    "prompt = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35730ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `joke_gen_chain`. ‚õìÔ∏è\n",
    "# This is a simple sequential chain that:\n",
    "# 1. Takes the initial `topic` input.\n",
    "# 2. Formats it using `prompt` to create the joke prompt.\n",
    "# 3. Sends the prompt to the `model` to generate the joke.\n",
    "# 4. Uses the `parser` to extract the raw joke string.\n",
    "# The output of this chain will be the joke text (e.g., \"Why did the AI go to the doctor? Because it had a byte!\").\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce6799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `parallel_chain`. üëØ\n",
    "# This uses `RunnableParallel` to run multiple operations concurrently on the *same input*.\n",
    "# The input to `parallel_chain` will be the *output of `joke_gen_chain`* (i.e., the joke text).\n",
    "# It creates a dictionary output with two keys: 'joke' and 'word_count'.\n",
    "parallel_chain = RunnableParallel({\n",
    "    # 'joke' key:\n",
    "    # `RunnablePassthrough()` takes the input it receives (the joke text from `joke_gen_chain`)\n",
    "    # and simply passes it through unchanged. So, the 'joke' key will contain the original joke text.\n",
    "    'joke': RunnablePassthrough(),\n",
    "\n",
    "    # 'word_count' key:\n",
    "    # `RunnableLambda(word_count)` wraps our custom `word_count` Python function,\n",
    "    # making it a Runnable. It receives the joke text as input and returns its word count.\n",
    "    'word_count': RunnableLambda(word_count)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3266a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `final_chain`. üîó\n",
    "# This chain combines `joke_gen_chain` and `parallel_chain` sequentially.\n",
    "# 1. `joke_gen_chain` runs first, generating the joke text.\n",
    "# 2. The *output* of `joke_gen_chain` (the joke text) becomes the *input* to `parallel_chain`.\n",
    "# 3. `parallel_chain` then executes its two branches concurrently using this joke text:\n",
    "#    - One branch just passes the joke through.\n",
    "#    - The other branch calculates the joke's word count using the `word_count` function.\n",
    "# 4. The final output of `final_chain` will be a dictionary like:\n",
    "#    `{'joke': '...', 'word_count': ...}`\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "# Invoke the `final_chain` with the initial topic. üöÄ\n",
    "# The input `{'topic':'AI'}` is fed to `joke_gen_chain` to start the process.\n",
    "result = final_chain.invoke({'topic':'AI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99857a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI go to therapy? \n",
      "\n",
      "Because it had too many unresolved issues and needed to reboot its emotional software! \n",
      " word count - 21\n"
     ]
    }
   ],
   "source": [
    "# Format the final output string using the results from the `final_chain`. üìä\n",
    "# This combines the joke text and its calculated word count into a readable format.\n",
    "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
    "\n",
    "# Print the formatted final result.\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
