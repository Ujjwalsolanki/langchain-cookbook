{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47523bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field # Pydantic for defining structured schemas\n",
    "\n",
    "# Load environment variables from a .env file. 🌍\n",
    "# This ensures sensitive information, like your Hugging Face API token,\n",
    "# is loaded securely from an external file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4503aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HuggingFaceEndpoint model. 🤖\n",
    "# This sets up the connection to the 'google/gemma-2-2b-it' model hosted on\n",
    "# the Hugging Face Inference API. This model is a relatively small, instruction-tuned LLM.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cde730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the HuggingFaceEndpoint with ChatHuggingFace. 💬\n",
    "# This adapter makes the Hugging Face model compatible with LangChain's\n",
    "# chat-specific interfaces, enabling it to handle conversational prompts.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfc5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic schema for the desired output. 📝\n",
    "# This `Person` class specifies that we expect an output object with:\n",
    "# - `name`: a string with a description.\n",
    "# - `age`: an integer, strictly greater than 18, with a description.\n",
    "# - `city`: a string with a description.\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person') # 'gt=18' enforces age > 18\n",
    "    city: str = Field(description='Name of the city the person belongs to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54734940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PydanticOutputParser. ⚙️\n",
    "# This parser is initialized with the `Person` Pydantic model. Its job is to:\n",
    "# 1. Generate text-based instructions for the LLM on how to format its output\n",
    "#    to match the `Person` schema.\n",
    "# 2. Attempt to parse the raw text output from the LLM into a `Person` object.\n",
    "#    If the LLM's output doesn't conform to the schema (e.g., not valid JSON,\n",
    "#    wrong types, missing fields), the parser will typically raise an error.\n",
    "parser = PydanticOutputParser(pydantic_object=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346650d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template. ✍️\n",
    "# - `template`: Contains the main instruction for the LLM.\n",
    "# - `input_variables`: Defines the dynamic parts of the prompt (here, 'place').\n",
    "# - `partial_variables`: This is crucial. It injects the `format_instruction`\n",
    "#    generated by the `PydanticOutputParser` into the prompt. This instruction\n",
    "#    tells the LLM *how* to format its response (e.g., \"Output should be a JSON\n",
    "#    object with keys 'name' (string), 'age' (integer > 18), 'city' (string).\").\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff397d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LangChain Expression Language (LCEL) chain. 🔗\n",
    "# The `|` operator pipes the output of one component to the input of the next.\n",
    "# 1. `template`: Takes `{'place':'sri lankan'}` and generates the full prompt string.\n",
    "# 2. `model`: Sends the prompt to the `google/gemma-2-2b-it` model.\n",
    "#    The model generates a text response based on the prompt, attempting to follow\n",
    "#    the format instructions (which is the challenging part for smaller models).\n",
    "# 3. `parser`: Receives the raw text output from the model and attempts to parse it\n",
    "#    into a `Person` Pydantic object. If the output isn't in the expected JSON format,\n",
    "#    this step will likely fail.\n",
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf8ac117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Anya Nova' age=32 city='Cygnus'\n"
     ]
    }
   ],
   "source": [
    "# Invoke the entire chain with the initial input. 🚀\n",
    "# The chain executes sequentially, aiming to produce a `Person` object.\n",
    "final_result = chain.invoke({'place':'Xandar'}) # Xandar: Gardians of the Galaxy fans will get it! 😉\n",
    "\n",
    "# Print the final result. 📊\n",
    "# If successful, `final_result` will be an instance of the `Person` Pydantic model.\n",
    "# If parsing fails due to the LLM's inability to generate structured output,\n",
    "# an error will be raised before this line is reached.\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
