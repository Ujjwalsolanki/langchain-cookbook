{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acbcecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6bca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable 'HF_HOME'.\n",
    "# This variable specifies the directory where Hugging Face models,\n",
    "# datasets, and other cached files will be stored.\n",
    "# By setting it, you control where the model weights are downloaded and saved.\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0fcadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\langchain-cookbook\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFacePipeline from a pre-trained model.\n",
    "# This approach directly loads a model into a Hugging Face pipeline,\n",
    "# often for local inference or when more fine-grained control over the pipeline\n",
    "# parameters is needed.\n",
    "# - 'model_id': Specifies the exact model to load from the Hugging Face Hub.\n",
    "#              \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" is a compact chat model.\n",
    "# - 'task': Defines the task the loaded model will perform, which is \"text-generation\".\n",
    "# - 'pipeline_kwargs': A dictionary to pass additional arguments directly to the\n",
    "#                      Hugging Face pipeline constructor.\n",
    "#   - 'temperature=0.5': Controls the randomness of the generated text. Lower values\n",
    "#                        (like 0.5) make the output more focused and deterministic.\n",
    "#   - 'max_new_tokens=100': Sets the maximum number of new tokens (words/subwords)\n",
    "#                           the model can generate in its response. This helps\n",
    "#                           limit the length of the output.\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5b8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the HuggingFacePipeline model with ChatHuggingFace.\n",
    "# ChatHuggingFace acts as an adapter, allowing the raw Hugging Face pipeline (llm)\n",
    "# to be used seamlessly within LangChain's chat-oriented interfaces. This means\n",
    "# you can send chat messages to it and receive responses.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b452382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the chat model with a specific question.\n",
    "# The prompt is \"What is the capital of France\".\n",
    "# The 'invoke' method sends this question to the model and waits for its answer.\n",
    "result = model.invoke(\"What is the capital of France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f58ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What is the capital of France</s>\n",
      "<|assistant|>\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Print the content of the model's response.\n",
    "# For chat models, the actual textual response is typically found within\n",
    "# the 'content' attribute of the returned message object.\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
