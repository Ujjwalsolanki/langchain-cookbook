{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7bcb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic # Not used in this specific example, but imported\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableBranch, RunnableLambda # Core LCEL components\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal # For defining literal types in Pydantic\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This ensures your OpenAI API key is loaded securely.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2943eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ü§ñ\n",
    "# This model will be used for both sentiment classification and response generation.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize a StrOutputParser. üìÑ\n",
    "# This parser is used to extract the raw string content from the final LLM response\n",
    "# in the conditional branches.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33519b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic schema for feedback sentiment. üìù\n",
    "# This `Feedback` class specifies that the sentiment must be either 'positive' or 'negative'.\n",
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd4482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PydanticOutputParser for the Feedback schema. ‚öôÔ∏è\n",
    "# This parser will generate instructions for the LLM to output sentiment as a structured object,\n",
    "# and then validate and parse the LLM's output into a `Feedback` object.\n",
    "parser2 = PydanticOutputParser(pydantic_object=Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da18cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for sentiment classification. ‚úçÔ∏è\n",
    "# It takes 'feedback' text and includes `format_instruction` from `parser2`\n",
    "# to guide the LLM to output the sentiment in the specified Pydantic format.\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Classify the sentiment of the following feedback text into positive or negative \\n {feedback} \\n {format_instruction}',\n",
    "    input_variables=['feedback'],\n",
    "    partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3d215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier chain. üîó\n",
    "# This chain takes feedback text, classifies its sentiment using the model,\n",
    "# and parses the output into a `Feedback` Pydantic object.\n",
    "classifier_chain = prompt1 | model | parser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b21fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for a positive response. üìù\n",
    "# This prompt takes the original 'feedback' text to craft a suitable positive reply.\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Write an appropriate response to this positive feedback \\n {feedback}',\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "\n",
    "# Define the prompt template for a negative response. üìù\n",
    "# This prompt takes the original 'feedback' text to craft a suitable negative reply.\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Write an appropriate response to this negative feedback \\n {feedback}',\n",
    "    input_variables=['feedback']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4a4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CONDITIONAL BRANCHING logic using RunnableBranch. üö¶\n",
    "# `RunnableBranch` takes a series of (condition, runnable) pairs and an optional default runnable.\n",
    "# - The first argument of each tuple is a `lambda` function that acts as the condition.\n",
    "#   It receives the output of the *previous* runnable in the overall chain (which is the `Feedback` object from `classifier_chain`).\n",
    "# - The second argument is the runnable to execute if the condition is true.\n",
    "#   Here, if `x.sentiment` is 'positive', `prompt2 | model | parser` is executed.\n",
    "#   If `x.sentiment` is 'negative', `prompt3 | model | parser` is executed.\n",
    "# - `RunnableLambda(lambda x: \"could not find sentiment\")` acts as a fallback if neither condition is met.\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x:x.sentiment == 'positive', prompt2 | model | parser),\n",
    "    (lambda x:x.sentiment == 'negative', prompt3 | model | parser),\n",
    "    RunnableLambda(lambda x: \"could not find sentiment\") # Default fallback if sentiment is not positive/negative\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96eb122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the classifier chain and the branching chain into a single, overall chain. üîó\n",
    "# The output of `classifier_chain` (the `Feedback` object) is passed directly to `branch_chain`.\n",
    "chain = classifier_chain | branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25e776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I'm sorry to hear that you had a negative experience. We strive to provide top-notch service and I would love to hear more about what specifically went wrong so we can address it and improve in the future. Please feel free to reach out to me directly so we can discuss this further. Thank you for bringing this to our attention.\"\n",
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "     +------------+      \n",
      "     | ChatOpenAI |      \n",
      "     +------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "# Invoke the entire chain with sample feedback. üöÄ\n",
    "# The chain will classify the sentiment and then generate an appropriate response based on it.\n",
    "print(chain.invoke({'feedback': 'This is a worst iphone'}))\n",
    "\n",
    "# Print an ASCII representation of the chain's graph. üìà\n",
    "# This visualization clearly shows the initial classification step,\n",
    "# followed by the branching logic that leads to different response generation paths.\n",
    "# It's an excellent tool for understanding the flow of conditional workflows.\n",
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
