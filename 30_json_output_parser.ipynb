{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa70ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser # For parsing JSON output\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This securely loads your Hugging Face API token or other credentials.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07039ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HuggingFaceEndpoint model. ü§ñ\n",
    "# This connects to 'google/gemma-2-2b-it' via the Hugging Face Inference API.\n",
    "# Gemma 2B is a relatively small, instruction-tuned model.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\" # Standard text generation task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the HuggingFaceEndpoint with ChatHuggingFace. üí¨\n",
    "# This makes the Hugging Face model compatible with LangChain's chat interfaces.\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8e669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a JsonOutputParser. ‚öôÔ∏è\n",
    "# This parser expects the LLM's raw text output to be a valid JSON string\n",
    "# and will attempt to convert it into a Python dictionary.\n",
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13540b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template. ‚úçÔ∏è\n",
    "# It asks the LLM to give 5 facts about a topic.\n",
    "# `format_instruction` will inject text instructions from the parser,\n",
    "# telling the LLM to output its response as a JSON object (e.g., {\"fact_1\": \"...\", ...}).\n",
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e7a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LangChain Expression Language (LCEL) chain. üîó\n",
    "# 1. `template`: Generates the full prompt string, including JSON formatting instructions.\n",
    "# 2. `model`: Sends the prompt to the Gemma 2B model. The model will try to generate\n",
    "#    text that looks like JSON, but its success in perfectly formatting it is limited.\n",
    "# 3. `parser`: Receives the raw text output from the model. It then attempts to parse\n",
    "#    this text as a JSON string into a Python dictionary. If the text isn't valid JSON,\n",
    "#    this step will likely raise an error (e.g., `json.decoder.JSONDecodeError`).\n",
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da936eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fact': 'Black holes are formed when massive stars collapse at the end of their lives.', 'explanation': 'When a star much larger than our Sun runs out of fuel, it cannot support itself against its own gravity. This leads to a supernova and the formation of a black hole: a region where gravity is so strong that nothing, not even light, can escape.'}, {'fact': \"Black holes have a 'singularity' at their center\", 'explanation': \"The singularity is a point of infinite density where all the mass of the black hole is concentrated. Our laws of physics break down at this point, and we can't fully describe what it is like.\"}, {'fact': \"It's impossible to see a black hole directly.\", 'explanation': 'The lack of visible light from black holes arises from their intense gravitational pull, which traps and prevents any light from escaping.  However, astronomers can detect them by observing their effects on surrounding matter.'}, {'fact': 'Black holes can have different sizes, ranging from stellar-mass black holes to supermassive black holes.', 'explanation': \"While we know them as stars with cosmic history and unique size, a black hole's size is also influenced by how much material it contains. The larger the material, and hence the mass, the stronger the gravitational pull and the 'larger' the black hole becomes\"}, {'fact': 'Scientists are still trying to understand the full nature and consequences of black holes.', 'explanation': 'These mysterious objects intrigue us because they are the ultimate mysteries of the universe. There are still many important questions to be answered about black holes, such as how they form, what happens inside them, and how they affect the universe.'}]\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the topic 'black hole'. üöÄ\n",
    "# This executes the full chain.\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "# Print the result. üìä\n",
    "# If the JSON parsing is successful (which is a big 'if' for this model),\n",
    "# `result` will be a Python dictionary containing the extracted facts.\n",
    "# Otherwise, you'll likely see an error message indicating a parsing failure.\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
