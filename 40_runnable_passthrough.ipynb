{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352c8c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough # Core LCEL components\n",
    "\n",
    "# Load environment variables from a .env file. üåç\n",
    "# This ensures your OpenAI API key is loaded securely from your environment.\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afd43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI language model. ü§ñ\n",
    "# This model will be used for both generating the joke and explaining it.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize a StrOutputParser. üìÑ\n",
    "# This parser extracts the raw string content from the LLM's response.\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874b00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first prompt template. üìù\n",
    "# This template takes a 'topic' as input and instructs the LLM to generate a joke about it.\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a dad-joke about {topic} in one sentence',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c430b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template. üìù\n",
    "# This template takes 'text' (which will be the joke generated in the previous step)\n",
    "# and instructs the LLM to explain that joke.\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text} in 2 sentences',\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bbbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first sub-chain: `joke_gen_chain`. ‚õìÔ∏è\n",
    "# This is a sequential chain that:\n",
    "# 1. Takes the initial `topic` input.\n",
    "# 2. Formats it using `prompt1` to create the joke prompt.\n",
    "# 3. Sends the prompt to the `model` to generate the joke.\n",
    "# 4. Uses the `parser` to extract the raw joke string.\n",
    "# The output of this chain will be the joke text (e.g., \"Why did the scarecrow win an award? Because he was outstanding in his field!\").\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfeb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parallel processing part: `parallel_chain`. üëØ\n",
    "# This uses `RunnableParallel` to run multiple operations concurrently.\n",
    "# The input to `parallel_chain` will be the *output of `joke_gen_chain`* (i.e., the joke text).\n",
    "# It creates a dictionary output with two keys: 'joke' and 'explanation'.\n",
    "parallel_chain = RunnableParallel({\n",
    "    # 'joke' key:\n",
    "    # `RunnablePassthrough()` takes the input it receives (which is the joke text from `joke_gen_chain`)\n",
    "    # and simply passes it through unchanged. So, the 'joke' key will contain the original joke text.\n",
    "    'joke': RunnablePassthrough(),\n",
    "\n",
    "    # 'explanation' key:\n",
    "    # This is another sequential chain that will:\n",
    "    # 1. Take the input (the joke text from `joke_gen_chain`).\n",
    "    # 2. Format it using `prompt2` (which asks to explain the joke).\n",
    "    # 3. Sends the explanation prompt to the `model`.\n",
    "    # 4. Uses the `parser` to extract the raw explanation string.\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea9bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the AI go to therapy? It had too many mental loops!', 'explanation': \"The joke plays on the idea that AI processes information through loops, but in this case, the AI's excessive mental loops led to the need for therapy. Essentially, the AI's repetitive thoughts or decision-making processes were causing it distress.\"}\n"
     ]
    }
   ],
   "source": [
    "# Define the final overall chain: `final_chain`. üîó\n",
    "# This chain combines `joke_gen_chain` and `parallel_chain` sequentially.\n",
    "# 1. `joke_gen_chain` runs first, generating the joke text.\n",
    "# 2. The *output* of `joke_gen_chain` (the joke text) becomes the *input* to `parallel_chain`.\n",
    "# 3. `parallel_chain` then executes its two branches concurrently using this joke text:\n",
    "#    - One branch just passes the joke through.\n",
    "#    - The other branch generates an explanation for the joke.\n",
    "# 4. The final output of `final_chain` will be a dictionary like:\n",
    "#    `{'joke': '...', 'explanation': '...'}`\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "# Invoke the entire `final_chain` with the initial topic. üöÄ\n",
    "# The input `{'topic':'cricket'}` is fed to `joke_gen_chain` to start the process.\n",
    "print(final_chain.invoke({'topic':'AI'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
